{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Day10-1. rnn_text_generation.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"display_name":"TensorFlow CPU","language":"python","name":"tesorflow2.0"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hcD2nPQvPOFM"},"source":["# 문자열 생성 (Text Generation) RNN 모델"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OZ6MrJN-bYhL","colab":{}},"source":["try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WGyKZj3bzf9p"},"source":["### 텐서플로와 다른 라이브러리 임포트"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"yG_n40gFzf9s","colab":{}},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np\n","import os\n","import time"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0g35LP0ubYhU","colab":{}},"source":["# 런타임에서 할당하는데 필요한 양만큼의 GPU 메모리를 할당\n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","if gpus:\n","  try:\n","    tf.config.experimental.set_memory_growth(gpus[0], True)\n","  except RuntimeError as e:\n","    # 프로그램 시작시에 메모리 증가가 설정되어야만 합니다\n","    print(e)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"EHDoRoc5PKWz"},"source":["### 셰익스피어 데이터셋 다운로드"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pD_55cOxLkAb","colab":{}},"source":["path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"PgEEOhFBbYhb"},"source":["### 데이터 전처리"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UHjdCjDuSvX_"},"source":["#### 데이터 로딩"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"aavnuByVymwK","colab":{}},"source":["# 읽은 다음 파이썬 2와 호환되도록 디코딩합니다.\n","text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n","# 텍스트의 길이는 그 안에 있는 문자의 수입니다.\n","print ('텍스트의 길이: {}자'.format(len(text)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Duhg9NrUymwO","colab":{}},"source":["# 텍스트의 처음 250자를 살펴봅니다\n","print(text[:250])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xUSssrYobYhk"},"source":["#### Vocabulary 생성"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"IlCgQBRVymwR","colab":{}},"source":["# 파일의 고유 문자수를 출력합니다.\n","vocab = sorted(set(text))\n","print ('고유 문자수 {}개'.format(len(vocab)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LFjSVAlWzf-N"},"source":["####  문자 별 인덱스, 인덱스 별 문자 맵핑 생성"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"IalZLbvOzf-F","colab":{}},"source":["# 문자에서 인덱스로 매핑\n","char2idx = {u:i for i, u in enumerate(vocab)}\n","\n","# 인덱스에서 문자로 매핑\n","idx2char = np.array(vocab)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kCVZComCbYhv"},"source":["#### 생성된 맵핑 확인 : char2idx 20개 항목 확인"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"FYyNlCNXymwY","colab":{}},"source":["print('{')\n","for char,_ in zip(char2idx, range(20)):\n","    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n","print('  ...\\n}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KwYf8VhqbYhr"},"source":["#### 문자열 데이터를 숫자열 데이터로 변환"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"C1sv4FyHbYhs","colab":{}},"source":["text_as_int = np.array([char2idx[c] for c in text])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"1rTLNqXdbYhz"},"source":["#### 문자열에서 숫자열로 맵핑 확인"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"l1VKcQHcymwb","colab":{}},"source":["# 텍스트에서 처음 13개의 문자가 숫자로 어떻게 매핑되었는지를 보여줍니다\n","print ('{} ---- 문자들이 다음의 정수로 매핑되었습니다 ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hgsVvVxnymwf"},"source":["### 데이터셋 생성"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"y6hl6WzsbYh3"},"source":["####  1. 문자 단위 데이터셋 생성"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0UHJDA39zf-O","colab":{}},"source":["# RNN 입력 sequence 길이\n","seq_length = 100\n","\n","# 데이터셋 만들기\n","char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n","\n","# 처음 5개 문자 확인\n","for i in char_dataset.take(5):\n","  print(idx2char[i.numpy()])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"O5WqSqnZbYh6"},"source":["#### 2. 청크 단위 데이터셋 생성"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"l4hkDU3i7ozi","colab":{}},"source":["# label 생성을 위해 배치 길이를 seq_length+1 설정\n","sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n","\n","# 처음 5개 sequence 확인\n","for item in sequences.take(5):\n","  print(repr(''.join(idx2char[item.numpy()])))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GoMEWw5ybYh-"},"source":["#### 3. 입력과 타겟이 분리된 데이터셋 생성"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9NGu-FkO_kYU","colab":{}},"source":["def split_input_target(chunk):\n","    input_text = chunk[:-1]\n","    target_text = chunk[1:]\n","    return input_text, target_text\n","\n","dataset = sequences.map(split_input_target)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hiCopyGZymwi"},"source":["#### 입력 & 타겟 확인"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GNbw-iR0ymwj","colab":{}},"source":["for input_example, target_example in  dataset.take(1):\n","  print ('입력 데이터: ', repr(''.join(idx2char[input_example.numpy()])))\n","  print ('타깃 데이터: ', repr(''.join(idx2char[target_example.numpy()])))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MJdfPmdqzf-R"},"source":["#### 4. 배치 단위의 데이터셋 생성"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"p2pGotuNzf-S","colab":{}},"source":["BATCH_SIZE = 64 # 배치 크기\n","BUFFER_SIZE = 10000 # 데이터셋을 섞을 버퍼 크기\n","\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5zrO6ngy4r0n","colab":{}},"source":["dataset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"r6oUuElIMgVx"},"source":["## 모델 정의"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"6H0TodK7bYiV"},"source":["#### 모델 정의"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MtCrdfzEI2N0","colab":{}},"source":["def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n","  model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n","                              batch_input_shape=[batch_size, None]),\n","    tf.keras.layers.LSTM(rnn_units,\n","                        return_sequences=True,\n","                        stateful=True,\n","                        recurrent_initializer='glorot_uniform'),\n","    tf.keras.layers.Dense(vocab_size)\n","  ])\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Bdekzq6ebYiY"},"source":["#### 모델 생성"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wwsrpOik5zhv","colab":{}},"source":["vocab_size = len(vocab) # 어휘 사전의 크기\n","embedding_dim = 256     # 임베딩 차원\n","rnn_units = 1024        # RNN 유닛(unit) 개수\n","\n","model = build_model(\n","  vocab_size = len(vocab),\n","  embedding_dim=embedding_dim,\n","  rnn_units=rnn_units,\n","  batch_size=BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-ubPo0_9Prjb"},"source":["#### 출력 Tensor Shape 확인"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"C-_70kKAPrPU","colab":{}},"source":["for input_example_batch, target_example_batch in dataset.take(1):\n","  example_batch_predictions = model(input_example_batch)\n","  print(example_batch_predictions.shape, \"# (배치 크기, 시퀀스 길이, 어휘 사전 크기)\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-4au2S3PbYig"},"source":["#### 모델 구조 확인"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vPGmAAXmVLGC","colab":{}},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UMU_kk2QbYii"},"source":["#### 예측 분포에서 샘플링 테스트\n","Categorical Distribution 샘플링"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4V4MfFg0RQJg","colab":{}},"source":["# (bach size, number of class)형태의 2D Tensor로 입력\n","# 따라서, 100개의 timestep이 batch인 것으로 처리됨\n","sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n","\n","# 출력이 (100,1)이므로 (100)으로 변경\n","sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GTaaoDL75VZk"},"source":["100개 timestep에 대한 샘플"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YqFMUQc_UFgM","colab":{}},"source":["sampled_indices"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"RqbioxI-bYin"},"source":["예측된 텍스트 복호화"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xWcFwPwLSo05","colab":{}},"source":["print(\"입력: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n","print()\n","print(\"예측된 다음 문자: \\n\", repr(\"\".join(idx2char[sampled_indices])))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LJL0Q0YPY6Ee"},"source":["## 모델 훈련"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2ttYgh06bYis"},"source":["#### 모델 컴파일"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DDl1_Een6rL0","colab":{}},"source":["model.compile(optimizer='adam', \n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ieSJdchZggUj"},"source":["#### 체크포인트 콜백 정의"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"W6fWTriUZP-n","colab":{}},"source":["# 체크포인트가 저장될 디렉토리\n","checkpoint_dir = './training_checkpoints'\n","# 체크포인트 파일 이름\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n","\n","checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_prefix,\n","    save_weights_only=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3Ky3F_BhgkTW"},"source":["#### 모델 훈련"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UK-hmKjYVoll","scrolled":true,"colab":{}},"source":["EPOCH = 30\n","\n","# 체크포인트 콜백 설정\n","model.fit(dataset, epochs=EPOCH, callbacks=[checkpoint_callback])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kKkD5M6eoSiN"},"source":["## 모델 테스트\n","모델을 테스트 하기 위해 훈련된 가중치를 갖고 배치 크기가 1인 입력을 받는 모델을 새로 만들어야 함"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"T3dShec0bYi7"},"source":["#### 테스트 용 모델을 새로 생성 (단, batch_size=1로 설정)\n","Hint : build_model() 함수를 사용해서 생성할 것"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"r9vtHlecAknJ","colab":{}},"source":["model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4EXb0woyAi8v"},"source":["#### 모델에 마지막 저장 체크포인트 복구 (Hint : model.load_weights() 이용)\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LycQ-ot_jjyu","colab":{}},"source":["last_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n","print(last_checkpoint)\n","model.load_weights(last_checkpoint)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jrNfyRL9BXkf"},"source":["#### 모델의 input shape을 [1, None]으로 변경"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tW0M74bHBTjH","colab":{}},"source":["# 배치 크기 1로 모델을 새로 빌드\n","model.build(tf.TensorShape([1, None]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"71xa6jnYVrAN","colab":{}},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DjGz1tDkzf-u"},"source":["#### 모델 테스트"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WvuwZBX5Ogfd","colab":{}},"source":["# 학습된 모델을 사용하여 텍스트 생성\n","def generate_text(model, start_string, num_generate):\n","  \n","  num_generate = 1000 # 생성할 문자의 수\n","\n","  # 시작 문자열을 숫자열로 변환\n","  input_eval = [char2idx[s] for s in start_string]\n","  input_eval = tf.expand_dims(input_eval, 0) # 2차원 배열로 변환\n","\n","  text_generated = [] # 생성된 결과를 저장할 빈 문자열\n","\n","  # temperature로 확률 값 조정 – 크면 균등분포, 낮으면 argmax와 같이 됨\n","  temperature = 1.0\n","\n","  model.reset_states()\n","  for i in range(num_generate):\n","      predictions = model(input_eval) # 배치 크기 = 1\n","      predictions = tf.squeeze(predictions, 0) # 배치 차원 제거\n","      predictions = predictions / temperature # temperature 적용\n","    \n","    \n","      # 범주형 분포를 사용하여 모델에서 리턴한 단어 예측\n","      # input   : [batch_size, num_classes]  RNN sequence를 batch 형태로 입력\n","      # output : [batch_size, num_samples] [-1,0]는 마지막 batch 항목에서 첫번째로 sampling한 값을 의미\n","      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n","\n","      # 예측된 단어를 다음 입력으로 모델에 전달\n","      input_eval = tf.expand_dims([predicted_id], 0) # 2차원 배열로 변환\n","\n","      text_generated.append(idx2char[predicted_id]) # 생성된 문자열에 추가\n","\n","  return (start_string + ''.join(text_generated))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ktovv0RFhrkn","colab":{}},"source":["num_generate = 1000 # 생성할 문자의 수\n","\n","print(generate_text(model, start_string=u\"ROMEO: \", num_generate=num_generate))"],"execution_count":null,"outputs":[]}]}